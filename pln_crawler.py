# -*- coding: utf-8 -*-
"""PLN Crawler

Automatically generated by Colaboratory.
"""

import pandas as pd
import requests
import re
from bs4 import BeautifulSoup

header = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:74.0) Gecko/20100101 Firefox/74.0'}

"""## General functions"""

def getLastPage(url, htmlClass, rawString, delStart, delEnd):
  findLastPage = re.compile(rawString)

  url = url + '1'
  html = requests.get(url, headers=header)
  bs = BeautifulSoup(html.text, "html.parser")
  lastPage = str(bs.find_all("a", class_=htmlClass))
  for page in findLastPage.finditer(lastPage):
    page = int(lastPage[page.start()+delStart:page.end()-delEnd])

  return page

def getRequests(url, lastPage):
  htmls = []
  for pageNum in range(1, lastPage+1):
    pageUrl = url + str(pageNum)
    html = requests.get(pageUrl, headers=header)
    htmls.append(html)
  
  return htmls

"""## Sensacionalista crawler
Otimizar
"""

# def getSensacionalistaData(sarcastic=True):
  # findAnchor = re.compile(r"<a[^<]*</a>")
  # findTitle = re.compile(r"title=\"[^\"]*")
  # findLink = re.compile(r"href=\"[^\"]*")

  url = "https://www.sensacionalista.com.br/pais/page/"

  # Get the maximum number of pages from sensacionalista.com.br/pais/page/1
  lastPage = getLastPage(url, "last", r"title=\"[^\"]*", 7, 0)
  print("[0]Total of {0} pages to crawl".format(lastPage))

  htmls = getRequests(url, lastPage)
  print("[1]All {0} pages requested successful".format(len(htmls)))
  
  # # Search trough all the pages and crawl the data
  # rawData = []
  # for html in htmls:
  #   bs = BeautifulSoup(html.text, "html.parser")
  #   div = str(bs.find_all("div", class_="td_module_8 td_module_wrap"))
  #   for anc in findAnchor.finditer(div):
  #     rawData.append(div[anc.start():anc.end()])
  # print("[2]Amount of {0} raw data".format(len(rawData)))

  # # Format the data to a dataframe
  # crawler = pd.DataFrame(columns=['article_link','headline','is_sarcastic'])
  # for data in rawData:
  #   dataList = [[],[],[]]
  #   for tmp in findLink.finditer(data):
  #     dataList[0] = data[tmp.start()+6:tmp.end()]
  #   for tmp in findTitle.finditer(data):
  #     dataList[1] = data[tmp.start()+7:tmp.end()]
  #   dataList[2] = sarcastic
  #   crawler.loc[len(crawler)] = dataList
  # print("[3]Dataframe formated and created")

  # return crawler

# Commented out IPython magic to ensure Python compatibility.
# %%time
# sensacionalista = getSensacionalistaData()

sensacionalista.to_csv("sensa.csv", sep=';', index=False, encoding='utf-8-sig')

"""## The Piauí Herald crawler
Em progresso
"""

# def getPiauiHerald(sarcastic=True):
#class="arquivo-lista tab-container blocos-column"
#HeraldAjax blocos-column

findAnchor = re.compile(r"<a href=[^>]*>\s<h2[^<]*")
findTitle = re.compile(r"<h2 class=\"bloco-title\">\\r\\n[ ]{35}")
findLink = re.compile(r"href=\"[^\"]*")
findYear = re.compile(r">\d{4}<")

# findYear = re.compile(r"data-tab=\"arquivo_\d*\">")

url = "https://piaui.folha.uol.com.br/herald/"
# htmls = []

# html = requests.get(url, headers=header)
# bs = BeautifulSoup(html.text, "html.parser")
# page = str(bs.find_all("div", class_="arquivo-lista tab-container blocos-column"))
# url = url[:-7]
# for year in findYear.finditer(page):
#   html = requests.get(url + page[year.start()+1:year.end()-1], headers=header)
#   htmls.append(html)

rawData = []
for html in htmls:
  bs = BeautifulSoup(html.text, "html.parser")
  div = str(bs.find_all("div", class_="inner"))
  for anc in findAnchor.finditer(div):
    rawData.append(div[anc.start():anc.end()])

crawler = pd.DataFrame(columns=['article_link','headline','is_sarcastic'])
for data in rawData:
  dataList = [[],[],[]]
  for tmp in findLink.finditer(data):
    dataList[0] = data[tmp.start()+6:tmp.end()]
  for tmp in findTitle.finditer(data):
    dataList[1] = data[tmp.start():tmp.end()]
  dataList[2] =  True
  crawler.loc[len(crawler)] = dataList

crawler

# rawData

rawData

"""## HuffPost Brasil crawler
Há fazer
https://www.huffpostbrasil.com/noticias/
"""

# def getHuffPostBrasilData(sarcastic=False):
url = "https://www.huffpostbrasil.com/noticias/"

lastPage = getLastPage(url, "pagination__link", r"href=\"/noticias/\d*/\"", 16, 2)
print("[0]Total of {0} pages to crawl".format(lastPage))

htmls = getRequests(url, lastPage)
print("[1]All {0} pages requested successful".format(len(htmls)))

findAnchor = re.compile(r"<a class=\"[^<]*</a>")

rawData = []
for html in htmls:
  bs = BeautifulSoup(html.text, "html.parser")
  div = str(bs.find_all("div", class_="apage-rail-cards"))
  for anc in findAnchor.finditer(div):
    rawData.append(div[anc.start():anc.end()])
print("[2]Amount of {0} raw data".format(len(rawData)))

def getRawData(htmls, htmlClass):
  rawData = []
  for html in htmls:
    bs = BeautifulSoup(html.text, "html.parser")
    div = str(bs.find_all("div", class_=htmlClass))

findLink = re.compile(r"href=\"[^\"]*\"")
findTitle = re.compile(r"target=\"_self\">[^<]*<")

crawler = pd.DataFrame(columns=["article_link","headline","is_sarcastic"])
for data in rawData:
  dataList = [[],[],[]]
  for tmp in findLink.finditer(data):
    dataList[0] = url[:-9] + data[tmp.start()+7:tmp.end()-1]
  for tmp in findTitle.finditer(data):
    dataList[1] = data[tmp.start()+15:tmp.end()-1]
  dataList[2] = False
  crawler.loc[len(crawler)] = dataList
crawler

crawler.to_csv("test.csv", sep=';', index=False, encoding="utf-8-sig")

"""## Há procura"""

