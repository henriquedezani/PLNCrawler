# -*- coding: utf-8 -*-
"""PLN Crawler

Automatically generated by Colaboratory.
"""

import pandas as pd
import requests
import re
from bs4 import BeautifulSoup

header = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:74.0) Gecko/20100101 Firefox/74.0'}

"""## General functions"""

def getLastPage(url, htmlClass, rawString, rmStart, rmEnd):
  findLastPage = re.compile(rawString)

  url = url + '1'
  html = requests.get(url, headers=header)
  bs = BeautifulSoup(html.text, "html.parser")
  lastPage = str(bs.find_all("a", class_=htmlClass))
  for page in findLastPage.finditer(lastPage):
    page = int(lastPage[page.start()+rmStart:page.end()-rmEnd])

  return page

def getRequests(url, lastPage):
  htmls = []
  
  for pageNum in range(1, lastPage+1):
    pageUrl = url + str(pageNum)
    html = requests.get(pageUrl, headers=header)
    htmls.append(html)
  
  return htmls

def getRawData(htmls, htmlClass, rawString, element="div"):
  findAnchor = re.compile(rawString)
  rawData = []

  for html in htmls:
    bs = BeautifulSoup(html.text, "html.parser")
    div = str(bs.find_all(element, class_=htmlClass))
    for anc in findAnchor.finditer(div):
      rawData.append(div[anc.start():anc.end()])
  
  return rawData

"""## Sensacionalista crawler"""

def getSensacionalistaData(sarcasm=True):
  url = "https://www.sensacionalista.com.br/pais/page/"

  lastPage = getLastPage(url, "last", r"title=\"[^\"]*", 7, 0)
  print("[0]Total of {0} pages to crawl".format(lastPage))

  htmls = getRequests(url, lastPage)
  print("[1]All {0} pages requested successful".format(len(htmls)))
  
  rawData = getRawData(htmls, "td_module_8 td_module_wrap", r"<a[^<]*</a>")
  print("[2]Amount of {0} raw data".format(len(rawData)))

  findLink = re.compile(r"href=\"[^\"]*")
  findTitle = re.compile(r"title=\"[^\"]*")
  
  dataFrame = pd.DataFrame(columns=['article_link','headline','is_sarcastic'])
  for data in rawData:
    dataList = [[],[],[]]
    for tmp in findLink.finditer(data):
      dataList[0] = data[tmp.start()+6:tmp.end()]
    for tmp in findTitle.finditer(data):
      dataList[1] = data[tmp.start()+7:tmp.end()]
    dataList[2] = sarcasm
    dataFrame.loc[len(dataFrame)] = dataList
  print("[3]Dataframe formated and created")

  return dataFrame

"""## The PiauÃ­ Herald crawler
Em progresso (me irritei e comecei novamente)
"""

def getPiauiHeraldData(sarcasm=True):
  url = "https://piaui.folha.uol.com.br/herald/"

  findYears = re.compile(r"data-tab=\"arquivo_\d{4}\">")
  findLink = re.compile(r"<a href=\"[^\"]*\">")
  findTitle = re.compile(r"<h2 class=\"bloco-title\">\s*.*")
  findTitle = re.compile(r"<h2 class=\"bloco-title\">\s*.*")
  
  htmls = []

  html = requests.get(url, headers=header)
  bs = BeautifulSoup(html.text, "html.parser")
  li = str(bs.find_all("li", class_="tab-btn"))
  for year in findYears.finditer(li):
    htmls.append(requests.get(str(url[:-7] + li[year.start()+18:year.end()-2]), headers=header))

  rawData = getRawData(htmls, "bloco size-2", r"<a href=[^>]*>\s<h2[^<]*")
  dataFrame = pd.DataFrame(columns=["article_link","headline","is_sarcastic"])
  for data in rawData:
    dataList = [[],[],[]]
    for tmp in findLink.finditer(data):
      dataList[0] = data[tmp.start()+9:tmp.end()-2]
    for tmp in findTitle.finditer(data):
      dataList[1] = data[tmp.start()+26:tmp.end()]
    if(dataList[1] == ""): continue
    dataList[2] = True
    dataFrame.loc[len(dataFrame)] = dataList

  return dataFrame

"""## HuffPost Brasil crawler"""

def getHuffPostBrasilData(sarcasm=False):
  url = "https://www.huffpostbrasil.com/noticias/"

  lastPage = getLastPage(url, "pagination__link", r"href=\"/noticias/\d*/\"", 16, 2)
  print("[0]Total of {0} pages to crawl".format(lastPage))

  htmls = getRequests(url, lastPage)
  print("[1]All {0} pages requested successful".format(len(htmls)))

  rawData = getRawData(htmls, "apage-rail-cards", r"<a class=\"[^<]*</a>")
  print("[2]Amount of {0} raw data".format(len(rawData)))

  findLink = re.compile(r"href=\"[^\"]*\"")
  findTitle = re.compile(r"target=\"_self\">[^<]*<")

  dataFrame = pd.DataFrame(columns=["article_link","headline","is_sarcastic"])
  for data in rawData:
    dataList = [[],[],[]]
    for tmp in findLink.finditer(data):
      dataList[0] = url[:-9] + data[tmp.start()+7:tmp.end()-1]
    for tmp in findTitle.finditer(data):
      dataList[1] = data[tmp.start()+15:tmp.end()-1]
    dataList[2] = sarcasm
    dataFrame.loc[len(dataFrame)] = dataList
  
  return dataFrame

"""## Nexo Jornal
Em progresso
"""

def getNexoJornalData(sarcasm=False):
  url = "https://www.nexojornal.com.br/tema/Sociedade?pagina="

  lastPage = getLastPage(url+"1", "Pagination__link___1VkYg", r">\d{3}</a>", 1, 4)

  htmls = getRequests(url, lastPage)

  rawData = getRawData(htmls, "Teaser__title-dark___1HEzZ", r"<a alt=\"[^>]*>", element="h4")

  findLink = re.compile(r"href=\"[^\"]*\"")
  findTitle = re.compile(r"title=\"[^\"]*\">")

  dataFrame = pd.DataFrame(columns=["article_link","headline","is_sarcastic"])
  for data in rawData:
    dataList = [[],[],[]]
    for tmp in findLink.finditer(data):
      dataList[0] = url[:-23] + data[tmp.start()+6:tmp.end()-1]
    for tmp in findTitle.finditer(data):
      dataList[1] = data[tmp.start()+7:tmp.end()-2]
    dataList[2] = sarcasm
    dataFrame.loc[len(dataFrame)] = dataList

  return dataFrame

"""## Main"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# sensacinalista = getSensacionalistaData()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# piauiherald = getPiauiHeraldData()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# huffpost = getHuffPostBrasilData()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# nexojornal = getNexoJornalData()

nexojornal